{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
 
    "\n",
    "Version: 2.0\n",
    "\n",
    "Environment: Python 3.8 and Anaconda 6.1.1 (64-bit)\n",
    "\n",
    "Libraries used:\n",
    "* os (For reading the data from the system)\n",
    "* langid (for check the language of the text)\n",
    "* nltk 3.2.2 (Natural Language Toolkit, included in Anaconda Python 3.6)\n",
    "* nltk.collocations (for finding bigrams, included in Anaconda Python 3.6)\n",
    "* nltk.tokenize (for tokenization, included in Anaconda Python 3.6)\n",
    "* nltk.corpus (for stop words, not included in Anaconda, `nltk.download('stopwords')` provided)\n",
    "\n",
    "\n",
    "\n",
    "## 1. Introduction\n",
    "\n",
    "This assignment comprises the execution of an Excel file regarding COVID-19 tweets. The excel file contains the 80+ days of COVID-19 related tweets. The excel file contains the 80+ sheets and each sheet has 2000 tweets.\n",
    "\n",
    "Each excel sheet contains information regarding tweets i.e `id`,`text`, and `created_at` attributes.\n",
    "\n",
    "The task requires the following steps:\n",
    "\n",
    "1. Extracting the data from the excel file.\n",
    "2. check whether the tweet text is English or not and filtered out the non-English tweets.\n",
    "3. Tokenize the data according to `[a-zA-Z]+(?:[-'][a-zA-Z]+)?`\n",
    "4. Generate the 100 most common bigrams and store the frequency of the bigrams into a text file.\n",
    "5. Generate the 100 most common unigrams and store the frequency of the unigram into a text file.\n",
    "6. Create a stored and indexed vocabulary list contain 200 most common bigrams and all the stemmed unigrams.\n",
    "7. Create a count vector-matrix based on the vocabulary list created in the previous step and store the frequency of each vector according to date into a text file. \n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import langid\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer \n",
    "import re\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Reading the data\n",
    "\n",
    "As a first step, all the excel files are loaded into Dataframe and then remove the unnecessary rows and columns. Then the text is extracted from the data frame and saved into a dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Reading the excel file and removing unwanted rows\n",
    "def format_data(data):\n",
    "    '''\n",
    "    Return text column from the excel file\n",
    "\n",
    "    Remove NaN and duplicate columns\n",
    "    '''\n",
    "    data.dropna(axis=0, how='all',inplace=True)\n",
    "    data.dropna(axis=1, how='all', inplace=True)\n",
    "    data.reset_index(drop=True,inplace=True)\n",
    "    data.drop(0,axis=0,inplace=True)\n",
    "    data.columns = ['text','id','created_at']\n",
    "    data.reset_index(drop=True,inplace=True)\n",
    "    data.drop_duplicates(inplace=True)\n",
    "    return data['text']\n",
    "\n",
    "# Reading the excel file\n",
    "# df = pd.ExcelFile('part2/part2/sample.xlsx')\n",
    "# df = pd.ExcelFile('30745012.xlsx')\n",
    "sheets = df.sheet_names\n",
    "data_dict = {sheet:format_data(df.parse(sheet)) for sheet in sheets}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Filtering out non-English tweets\n",
    "\n",
    "Langid package is used to remove the non-English tweets and store the data into a dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Filter out the non-English text \n",
    "data_dict = dict(map(lambda x: (x[0],[word for word in x[1] if langid.classify(str(word).encode('utf-16','surrogatepass').decode('utf-16'))[0] =='en']),data_dict.items()))\n",
    "\n",
    "data_dict_str = dict(map( lambda x:(x[0], ' '.join(map(str, x[1]))), data_dict.items() ))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.Tokenizing the data\n",
    "\n",
    "The data is tokenized using `RegexTokenizer` and store the data into a dictionary where the key is the date and tokenized text as value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tokenizer = RegexpTokenizer(r\"[a-zA-Z]+(?:[-'][a-zA-Z]+)?\")\n",
    "\n",
    "def tokenised_text(key,text):\n",
    "    '''\n",
    "    return the key and token list \n",
    "\n",
    "    key =  dates\n",
    "    text =  twitter text in string format\n",
    "    '''\n",
    "    tokens = tokenizer.tokenize(str(text).replace('\\n',' ').lower())\n",
    "    return (key, tokens)\n",
    "\n",
    "\n",
    "tokenised_data = dict(tokenised_text(key, value) for key,value in data_dict_str.items())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.Generating the 100 most common bigrams\n",
    "The tokenized dictionary is used to generate 100 most common bigrams and store the frequency of bigrams into a text file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.util import ngrams\n",
    "from nltk.probability import *\n",
    "\n",
    "def generate_bigram(token_list):\n",
    "    \"\"\"\n",
    "    return the 100 most common bigrams\n",
    "    input: token_list for each date\n",
    "    \"\"\"\n",
    "    return FreqDist(ngrams(token_list, n=2)).most_common(100)\n",
    "\n",
    "bigrams = dict(map( lambda x : (x[0], generate_bigram(x[1])), tokenised_data.items() ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def write_file(file_name, data):\n",
    "    '''\n",
    "    write the data into the given file_name\n",
    "\n",
    "    input:  file Name and data\n",
    "\n",
    "    '''\n",
    "    with open(file_name, 'w') as f:\n",
    "        for key,value  in data.items():\n",
    "            f.write(\"{}:{}\".format(key,value))\n",
    "            f.write('\\n')\n",
    "\n",
    "write_file('30745012_100bi.txt', bigrams)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Generating the unigrams\n",
    "### 7.1. Removing stop word and stemming \n",
    "In order to generate 100 most common unigram, we have to remove the stop word and stemmed the data using porter stemmer.\n",
    "\n",
    "Then `FreqDist` is used to calculate the frequency of each token and store the result into a text file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Read the stop word file\n",
    "file = open('part2/stopwords_en.txt','r')\n",
    "context_independent_stop_words  = file.read().split('\\n')\n",
    "file.close()\n",
    "# context_independent_stop_words = context_independent_stop_words\n",
    "#creating the porter stemmer object \n",
    "ps = nltk.PorterStemmer()\n",
    "\n",
    "\n",
    "def remove_stop_word(key,value):\n",
    "    ''''\n",
    "    return the key value pair after removing the stop and applying stemming\n",
    "\n",
    "    '''\n",
    "    tokens =  list(filter(lambda x: len(x)>=3, value))\n",
    "    tokens =  list(filter(lambda x : x not in context_independent_stop_words, tokens))\n",
    "    return (key,tokens)\n",
    "\n",
    "def apply_stemming(key, value):\n",
    "    \"\"\"\n",
    "    return the key value pair after applying stemming\n",
    "\n",
    "    \"\"\"\n",
    "    stemmed_list = list(map(lambda x:ps.stem(x),value)) \n",
    "    return (key,stemmed_list)\n",
    "\n",
    "text_without_stop_word = dict(remove_stop_word(key,value) for key,value in tokenised_data.items())\n",
    "\n",
    "stemmed_text = dict(apply_stemming(key,value) for key,value in text_without_stop_word.items())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_unigram(token_list):\n",
    "    '''\n",
    "    return: 100 most common unigrams\n",
    "    \n",
    "    input: list of token for each date\n",
    "    '''\n",
    "    return FreqDist(token_list).most_common(100)\n",
    "    \n",
    "\n",
    "unigrams = dict(map( lambda x:(x[0], generate_unigram(x[1])), stemmed_text.items() ))\n",
    "write_file('30745012_uni.txt', unigrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Creating the vocabulary\n",
    "\n",
    "To create Vocabulary:\n",
    "- **Generate 200 most common bigram using ` nltk.collocations.BigramAssocMeasures` function through PMI measure**\n",
    "- **Generate unigram after removal of dependent and independent stopword and then stemmed the tokenized data**\n",
    "- ** Concatenate the unigram and bigram and store the sorted result into the text file.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "from itertools import chain\n",
    "from nltk.tokenize import MWETokenizer\n",
    "\n",
    "# generating the unique date wise text in the dictionary\n",
    "unique_date_wise_text = dict((key,set(value)) for key,value in text_without_stop_word.items())\n",
    "\n",
    "# combining the dictionary value and store into list\n",
    "word_list = list(chain.from_iterable(unique_date_wise_text.values()))\n",
    "# calculating the the frequency of each token\n",
    "word_freq = FreqDist(word_list)\n",
    "# filtering the token whose document frequency is less than 5 and greater than 60\n",
    "word_freq = dict(filter(lambda x : x[1]>=5 and x[1]<=60, word_freq.items()))\n",
    "# applying the porter stemmer \n",
    "words = list(map(lambda x : ps.stem(x), word_freq.keys()))\n",
    "\n",
    "\n",
    "# generating the bigrams according to pmi measure\n",
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "bi_token = list(chain.from_iterable(tokenised_data.values()))\n",
    "finder = nltk.collocations.BigramCollocationFinder.from_words(bi_token)\n",
    "pmi_bigram_200 = finder.nbest(bigram_measures.pmi, 200)\n",
    "\n",
    "mwe_tokenizer = MWETokenizer(pmi_bigram_200)\n",
    "\n",
    "def mwe_generator(token_list):\n",
    "    \n",
    "    mwe_token =  mwe_tokenizer.tokenize(token_list)\n",
    "    bigram_tokens  = list(filter(lambda x: '_' in x, mwe_token)) \n",
    "    return bigram_tokens\n",
    "\n",
    "mwe_tokens = list(map(lambda x: mwe_generator(x), tokenised_data.values()))\n",
    "mwe_tokens = list(set(chain.from_iterable(mwe_tokens)))\n",
    "\n",
    "# # adding unigram and bigram\n",
    "vocab = list(set(words)) + mwe_tokens\n",
    "\n",
    "vocab.sort()\n",
    "with open('30745012_vocab.txt','w') as f:\n",
    "    for index,word in enumerate(vocab):\n",
    "        f.write(\"{}:{}\".format(word,index))\n",
    "        f.write('\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Creating a count vector\n",
    "This task consists of extracting the tweets and calculating its corresponding sparse count vector. \n",
    "The count vector is a collection of words, frequency pairs that count the number of occurrences of every word in the data. \n",
    "\n",
    "The word is used in spare matrics is extracted according to the following rules:\n",
    "\n",
    "1. 200 most common bigram are generated using `PMI measure` and `MWETokenizer` is used to tokenize the bigram from the list of words\n",
    "2. Unigrams are generated after removing context-dependent and independent stop words and stemming the word.\n",
    "3. Then the vocabulary list is used to create count vectors.\n",
    "4. After that tocoo() function is used to extract the row, columns, and data information from the sparse matrix.\n",
    "5. In the end, the data is stored in a text file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import MWETokenizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "with open('30745012_vocab.txt','r') as f:\n",
    "    vocab = f.read().split('\\n')[:-1]\n",
    "vocab = {x.split(':')[0]:x.split(':')[1] for x in vocab }\n",
    "vocab\n",
    "\n",
    "mwe_tokenizer = MWETokenizer(pmi_bigram_200)\n",
    "def mwe_generator(token_list):\n",
    "    \n",
    "    mwe_token =  mwe_tokenizer.tokenize(token_list)\n",
    "    mwe_token = list(filter(lambda x: len(x)>=3, mwe_token))\n",
    "    mwe_token = list(filter(lambda x: x not in context_independent_stop_words, mwe_token))\n",
    "\n",
    "    bigram_tokens  = list(filter(lambda x: '_' in x, mwe_token)) \n",
    "    unigram_tokens = [ps.stem(x) for x in mwe_token if '_' not in x]\n",
    "    mwe_token =  bigram_tokens + unigram_tokens\n",
    "    return mwe_token\n",
    "mwe_tokens = dict(map(lambda x: (x[0],mwe_generator(x[1])), tokenised_data.items()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8, 1836)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer(analyzer = \"word\",vocabulary = vocab.keys() ) \n",
    "data_features = vectorizer.fit_transform([' '.join(value) for value in mwe_tokens.values()])\n",
    "print(data_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dates = list(mwe_tokens.keys())\n",
    "import itertools\n",
    "def generate_counvt_vec_file(features):\n",
    "    cx = features.tocoo()\n",
    "    count_vector = []\n",
    "    for i,j,v in itertools.zip_longest(cx.row, cx.col, cx.data):\n",
    "        count_vector.append((dates[i],str(j), str(v)))\n",
    "\n",
    "    count_vector = pd.DataFrame(count_vector, columns=['date','token','count'])\n",
    "    with open('30745012_countVect.txt','w') as f:\n",
    "        for date in dates:\n",
    "            token = count_vector[count_vector.date == date][['token','count']].set_index('token').to_dict()['count']\n",
    "            f.write(\"{},\".format(date))\n",
    "            length = len(token)\n",
    "            i = 1\n",
    "            for k,v in token.items():\n",
    "                if(i<length):\n",
    "                    \n",
    "                    f.write(\"{}:{},\".format(k,v))\n",
    "                else:\n",
    "                    f.write(\"{}:{}\".format(k,v))\n",
    "                i = i+1\n",
    "            f.write(\"\\n\")\n",
    "generate_counvt_vec_file(data_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10 Summary\n",
    "This task uses the natural language preprocessing toolkit to extract useful information regarding the COVID-19 twitter data set.\n",
    "The main outcomes achieved while applying these techniques were:\n",
    "\n",
    "\n",
    "- **Data extraction**: Reading the Excel file in python and store the data into a pandas data frame.\n",
    "- **Data frame manipulation**: By using the `pandas` package,\n",
    "removing NaN rows and columns then convert the pandas data frame into a dictionary.\n",
    "- **Exporting data to specific format**: By using built-in functions like `open` it was possible to export dictionary into `.txt` files with additional formatting and transformations. \n",
    "- **Tokenization, collocation extraction**.` RegexpTokenizer's function is used to tokenize the data and obtain letter only words.\n",
    "\n",
    "- **Stop word**: Context-dependent and independent stopwords are removed from the data.\n",
    "- **Stemming**: `Porter stemmer` is used to stem the tokenized words.\n",
    "- **Bigrams**: For bigram generation, MWEtokenizer and PMI measure is used and generate 200 most common bigrams.\n",
    "\n",
    "- **Vocabulary and sparse vector generation**. A vocabulary covering words from different abstracts was obtained by removing stop words, the top 200 most frequent ones. Filtering based on `nltk`'s frequency distribution function `FreqDist()` and also the built-in functions `set()` and `enumerate()` were used to get the final vocabulary dictionary. Finally, a sparse vector was calculated for every abstract by counting the frequency of vocabulary word occurrences.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
